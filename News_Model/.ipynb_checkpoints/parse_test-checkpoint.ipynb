{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def base_parser(text, base_site):\n",
    "\n",
    "    site_list = []\n",
    "    \n",
    "    if base_site in site_list:\n",
    "        #Use specific parser\n",
    "        print('Using Specific Parser')\n",
    "    \n",
    "    else:\n",
    "    \n",
    "        output = ''\n",
    "        blacklist = [\n",
    "            '[document]',\n",
    "             'a',\n",
    "             'aside',\n",
    "             'article'\n",
    "             'b',\n",
    "             'body',\n",
    "             'div',\n",
    "             'fieldset',\n",
    "             'figure',\n",
    "             'footer',\n",
    "             'form',\n",
    "             'h1',\n",
    "             'h2',\n",
    "             'h6',\n",
    "             'head',\n",
    "             'header',\n",
    "             'html',\n",
    "             'input',\n",
    "             'li',\n",
    "             'link',\n",
    "             'meta',\n",
    "             'noscript',\n",
    "             'script',\n",
    "             'section',\n",
    "             'span',\n",
    "             'strong',\n",
    "             'style',\n",
    "             'time',\n",
    "             'title',\n",
    "             'ul',\n",
    "             'button',\n",
    "             'svg']\n",
    "\n",
    "        test = ''\n",
    "        \n",
    "        for t in text:\n",
    "            if t.parent.name not in blacklist:\n",
    "                \n",
    "                test += t.parent.name\n",
    "                \n",
    "                output += '{} '.format(t)\n",
    "\n",
    "        final = output.replace('\\n','')\n",
    "    \n",
    "    return final\n",
    "\n",
    "\n",
    "def time_parser(soup, base_site):\n",
    "    \n",
    "    #Convert all dates/times to datetime format.... \n",
    "    \n",
    "    site_list = ['www.sportsnet.ca','www.nhl.com','www.tsn.ca','thehockeywriters.com','globalnews.ca']\n",
    "    \n",
    "    if base_site in site_list:\n",
    "        \n",
    "                            \n",
    "        if base_site == 'globalnews.ca':\n",
    "            \n",
    "            spans = soup.find_all('div', {'class' : 'c-byline__date c-byline__date--pubDate toggle-switch'})\n",
    "            \n",
    "            lines = [span.get_text() for span in spans]\n",
    "            \n",
    "            final = lines[0]                     \n",
    "        \n",
    "        \n",
    "        if base_site == 'www.sportsnet.ca':\n",
    "            \n",
    "            spans = soup.find_all('span', {'class' : 'article-publish-date'})\n",
    "            \n",
    "            lines = [span.get_text() for span in spans]\n",
    "            \n",
    "            if len(lines)>0:\n",
    "                final = lines[0]\n",
    "            \n",
    "            else:\n",
    "                final = 'NaN'\n",
    "            \n",
    "        if base_site == 'thehockeywriters.com':\n",
    "            \n",
    "            spans = soup.find_all('span', {'itemprop' : 'dateCreated'})\n",
    "            \n",
    "            lines = [span.get_text() for span in spans]\n",
    "            \n",
    "            final = lines[0]\n",
    "            \n",
    "            \n",
    "        if base_site == 'www.nhl.com':\n",
    "            \n",
    "            spans = soup.find_all('span', {'class' : 'article-item__date'})\n",
    "            \n",
    "            print(spans)\n",
    "            \n",
    "            lines = [span.attrs['data-date'] for span in spans]\n",
    "            \n",
    "            final = lines[0]\n",
    "            \n",
    "        if base_site == 'www.tsn.ca':\n",
    "            \n",
    "            spans = soup.find_all('div', {'class' : 'date'})\n",
    "            \n",
    "            print('test')\n",
    "            \n",
    "            lines = [span.get_text() for span in spans]\n",
    "            \n",
    "            final = lines[0].replace('\\n','').strip()\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        final = 'Nan'    \n",
    "        for i in soup.findAll('time'):\n",
    "            if i.has_attr('datetime'):\n",
    "                final = i['datetime']\n",
    "    \n",
    "    return final\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = 'https://globalnews.ca/news/7251749/beirut-explosion-montreal-resident-killed/'\n",
    "\n",
    "url = link.split('&')[0]\n",
    "    \n",
    "res = requests.get(url)\n",
    "html_page = res.content\n",
    "soup = BeautifulSoup(html_page, 'html.parser')\n",
    "\n",
    "base_site = url.split('/')[2]\n",
    "\n",
    "time = time_parser(soup,base_site)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPosted August 5, 2020 12:39 pm\\n\\n\\xa0\\n\\xa0\\n\\n'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
